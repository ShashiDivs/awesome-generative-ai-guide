# :star2: Most Impactful AI Evaluation Papers 
### (January 2025 to October 2025)

The field of AI evaluation has experienced growth in 2025, driven by the need for robust, comprehensive, and reliable assessment methods for AI systems. As Large Language Models (LLMs) and Multimodal AI systems become more capable and widely deployed, the development of effective evaluation frameworks has become important for ensuring their quality, safety, and reliability. This table provides summaries of papers published in 2025, covering various aspects of AI evaluation research. These topics include:

1. **LLM Judges & Automated Evaluation**: Methods for using AI systems to evaluate other AI systems, including LLM-as-a-judge approaches.
2. **Benchmarks & Datasets**: Comprehensive evaluation benchmarks and datasets for assessing AI model performance.
3. **Domain-Specific Evaluation**: Specialized evaluation frameworks tailored for specific domains like finance, science, and medicine.
4. **Multimodal Evaluation**: Assessment methods for vision-language, audio-visual, and other multimodal AI systems.
5. **Reasoning & Cognitive Evaluation**: Frameworks for evaluating reasoning, logic, and cognitive capabilities in AI models.
6. **Evaluation Methodologies**: Novel approaches and frameworks for AI system assessment.
7. **Robustness & Safety Evaluation**: Methods for assessing model robustness, safety, and reliability.
8. **Evaluation Surveys**: Comprehensive reviews and surveys of evaluation approaches and methodologies.

This table will continue to be updated regularly as new evaluation research emerges!

| Title | Description | Tags | Month |
|-------|-------------|------|------|
| [UniGenBench++: A Unified Semantic Evaluation Benchmark for Text-to-Image Generation](https://arxiv.org/abs/2510.15114) | This paper introduces UniGenBench++, a comprehensive benchmark for evaluating text-to-image generation models through unified semantic evaluation metrics. The framework provides standardized assessment methods for measuring how well generative models capture semantic meaning from text prompts, addressing the challenge of consistent evaluation across different text-to-image systems. The benchmark enables more reliable comparisons between models and helps identify specific areas where improvements are needed in semantic understanding and visual generation. | Benchmarks & Datasets | October 2025 |
| [OmniVideoBench: Towards Audio-Visual Understanding Evaluation for Omni MLLMs](https://arxiv.org/abs/2510.08863) | This paper presents OmniVideoBench, a comprehensive evaluation framework specifically designed for assessing audio-visual understanding capabilities in omni multimodal large language models. The benchmark addresses the growing need for robust evaluation of models that can process and understand both audio and visual information simultaneously, providing standardized metrics for measuring performance across various audio-visual tasks and scenarios that require integrated multimodal reasoning. | Multimodal Evaluation | October 2025 |
| [BEAR: Benchmarking and Enhancing Multimodal Language Models for Atomic Embodied Capabilities](https://arxiv.org/abs/2510.08597) | BEAR introduces a comprehensive benchmark for evaluating and improving multimodal language models' atomic embodied capabilities. The framework focuses on testing fundamental skills required for embodied AI, such as spatial reasoning, object manipulation understanding, and physical world comprehension. By breaking down complex embodied tasks into atomic components, BEAR provides detailed insights into which specific capabilities need improvement in multimodal models for real-world applications. | Multimodal Evaluation | October 2025 |
| [CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and Outcome Reward](https://arxiv.org/abs/2508.03686) | CompassVerifier addresses fundamental limitations in LLM evaluation by introducing a unified and robust verification system for answer evaluation and outcome rewards. The system demonstrates multi-domain competency across math, knowledge, and reasoning tasks, processing various answer types including multi-subproblems and formulas. Built on the comprehensive VerifierBench benchmark with 1 million expert-labeled predictions, CompassVerifier-7B achieves state-of-the-art performance while being robust to different prompt styles and capable of identifying invalid responses. | LLM Judges & Automated Evaluation | August 2025 |
| [From Scores to Skills: A Cognitive Diagnosis Framework for Evaluating Financial Large Language Models](https://arxiv.org/abs/2508.04307) | This paper introduces a cognitive diagnosis framework that moves beyond traditional scoring methods to provide detailed skill-based evaluation of financial Large Language Models. The approach identifies specific cognitive abilities and knowledge gaps in financial reasoning, offering more granular insights into model performance than aggregate scores. The framework enables targeted improvements by pinpointing which financial concepts and reasoning skills need enhancement in LLM training and development. | Domain-Specific Evaluation | August 2025 |
| [SciArena: An Open Evaluation Platform for Foundation Models in Scientific Literature Tasks](https://arxiv.org/abs/2507.06981) | SciArena establishes an open evaluation platform specifically designed for assessing foundation models' performance on scientific literature tasks. The platform provides comprehensive benchmarks covering various aspects of scientific text processing, including literature review, hypothesis generation, and scientific reasoning. By creating standardized evaluation protocols for scientific applications, SciArena enables researchers to systematically compare model performance and identify areas for improvement in scientific AI applications. | Domain-Specific Evaluation | July 2025 |
| [VS-Bench: Evaluating VLMs for Strategic Reasoning and Decision-Making in Multi-Agent Environments](https://arxiv.org/abs/2506.05821) | VS-Bench introduces a specialized benchmark for evaluating Vision-Language Models' capabilities in strategic reasoning and decision-making within multi-agent environments. The framework tests models' ability to understand complex strategic interactions, predict opponent behaviors, and make optimal decisions based on visual and textual information. This benchmark addresses the growing need for AI systems that can operate effectively in competitive and collaborative multi-agent scenarios. | Reasoning & Cognitive Evaluation | June 2025 |
| [VideoReasonBench: Can MLLMs Perform Vision-Centric Complex Video Reasoning?](https://arxiv.org/abs/2505.18696) | VideoReasonBench creates a comprehensive evaluation framework for testing Multimodal Large Language Models' ability to perform complex reasoning over video content. The benchmark focuses on vision-centric reasoning tasks that require understanding temporal relationships, spatial dynamics, and causal connections in video sequences. By emphasizing visual reasoning over textual shortcuts, the benchmark provides a rigorous assessment of models' true video understanding capabilities. | Multimodal Evaluation | May 2025 |
| [VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal Large Language Models](https://arxiv.org/abs/2504.15279) | VisuLogic addresses visual reasoning evaluation by introducing a benchmark of 1,000 human-verified problems across six reasoning categories. The benchmark employs an anti-linguistic shortcut design to ensure tasks require visual reasoning rather than text-based shortcuts. With most models scoring below 30% compared to 51.4% human performance, VisuLogic shows gaps in current MLLMs' visual reasoning capabilities and suggests reinforcement learning as a potential enhancement approach. | Multimodal Evaluation | April 2025 |
| [Survey on Evaluation of LLM-based Agents](https://arxiv.org/abs/2503.16416) | This comprehensive survey examines the current landscape of evaluation methodologies for LLM-based agents, providing a systematic overview of existing approaches, challenges, and future directions. The survey categorizes different evaluation frameworks, discusses their strengths and limitations, and identifies gaps in current assessment methods for agent-based AI systems. It serves as a foundational resource for researchers developing new evaluation approaches for autonomous AI agents. | Evaluation Surveys | March 2025 |
| [Preference Leakage: A Contamination Problem in LLM-as-a-judge](https://arxiv.org/abs/2502.01534) | This paper identifies a bias issue in LLM-as-a-judge systems called "preference leakage," where judges show bias toward models they are related to through shared architecture, inheritance, or family relationships. Through experiments across multiple benchmarks, the research demonstrates that this contamination problem is present and can be harder to detect than previously identified biases, affecting the reliability of LLM-based evaluation systems. The work provides theoretical analysis and empirical evidence of this evaluation challenge. | LLM Judges & Automated Evaluation | February 2025 |
| [SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines](https://arxiv.org/abs/2502.14739) | SuperGPQA introduces an evaluation benchmark that scales LLM assessment across 285 graduate-level disciplines. Using a Human-LLM collaborative filtering mechanism with over 80 expert annotators, the benchmark addresses the evaluation of LLMs across specialized fields including light industry, agriculture, and service-oriented disciplines. Results show DeepSeek-R1 achieving accuracy of 61.82%, indicating room for improvement in current models. | Benchmarks & Datasets | February 2025 |
| [LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models](https://arxiv.org/abs/2510.20608) | LIBERO-Plus conducts robustness analysis of Vision-Language-Action models, examining their performance under various challenging conditions and perturbations. The framework evaluates how well these models maintain performance when faced with visual noise, language ambiguity, and environmental variations, providing insights for deploying VLA models in real-world scenarios where robustness is important. | Robustness & Safety Evaluation | October 2025 |
| [When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs](https://arxiv.org/abs/2510.14408) | This paper explores evaluation methodologies for assessing reasoning capabilities in long-context language models, focusing on how models integrate factual information with reasoning processes. The work examines the challenges of evaluating reasoning consistency across extended contexts and proposes methods for measuring the reusability of reasoning patterns in large-scale language understanding tasks. | Reasoning & Cognitive Evaluation | October 2025 |
| [When Does Reasoning Matter? A Controlled Study of Reasoning's Contribution to Model Performance](https://arxiv.org/abs/2509.06471) | This controlled study systematically evaluates the contribution of reasoning capabilities to overall model performance across various tasks. The research provides empirical evidence for when and how reasoning mechanisms improve model outputs, offering valuable insights for designing evaluation frameworks that accurately measure the impact of different reasoning approaches on AI system performance. | Reasoning & Cognitive Evaluation | September 2025 |
| [ReviewScore: Misinformed Peer Review Detection with Large Language Models](https://arxiv.org/abs/2509.11032) | ReviewScore introduces a novel application of LLM evaluation by using large language models to detect misinformed or problematic peer reviews in academic settings. The system evaluates the quality, accuracy, and constructiveness of peer reviews, demonstrating how AI evaluation can be applied to improve academic processes and ensure higher quality scholarly communication. | LLM Judges & Automated Evaluation | September 2025 |
| [SWE-QA: Can Language Models Answer Repository-level Code Questions?](https://arxiv.org/abs/2509.09863) | SWE-QA creates an evaluation framework for testing language models' ability to understand and answer questions about software repositories. The benchmark evaluates models' capacity to comprehend large codebases, understand software architecture, and provide accurate responses to repository-level queries, addressing code understanding evaluation needs. | Domain-Specific Evaluation | September 2025 |
| [Inverse IFEval: Can LLMs Unlearn Stubborn Training Conventions to Follow Real Instructions?](https://arxiv.org/abs/2509.07703) | This paper introduces Inverse IFEval, which evaluates LLMs' ability to overcome ingrained training patterns and follow new, potentially contradictory instructions. The evaluation framework tests models' flexibility in adapting to instructions that conflict with their training conventions, providing insights into instruction-following capabilities and training bias effects. | Evaluation Methodologies | September 2025 |
| [Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth](https://arxiv.org/abs/2509.07749) | Drivel-ology presents a unique evaluation approach that tests LLMs' reasoning capabilities by challenging them to interpret and analyze nonsensical text with apparent depth. This benchmark evaluates models' ability to distinguish meaningful reasoning from superficial pattern matching, revealing important insights about the robustness of AI reasoning systems. | Reasoning & Cognitive Evaluation | September 2025 |
| [A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers](https://arxiv.org/abs/2509.06942) | This comprehensive survey examines evaluation methodologies for scientific Large Language Models, covering assessment approaches from data foundations to autonomous agent applications. The survey provides a systematic overview of evaluation challenges and solutions specific to scientific AI applications, serving as a foundational resource for researchers in scientific AI evaluation. | Evaluation Surveys | September 2025 |
| [When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs](https://arxiv.org/abs/2508.09282) | This study conducts a comparison of methods for evaluating and improving prompt robustness in LLMs, with focus on punctuation sensitivity. The research shows how textual variations can impact model performance and provides evaluation frameworks for measuring and improving prompt robustness across different LLM architectures. | Robustness & Safety Evaluation | August 2025 |
| [Has GPT-5 Achieved Spatial Intelligence? An Empirical Study](https://arxiv.org/abs/2508.12026) | This empirical study provides comprehensive evaluation of GPT-5's spatial intelligence capabilities through systematic testing across multiple spatial reasoning tasks. The evaluation covers spatial relationship understanding, 3D reasoning, and geometric problem-solving, offering insights into the current state of spatial intelligence in advanced language models and identifying areas for improvement. | Reasoning & Cognitive Evaluation | August 2025 |
| [CMPhysBench: A Benchmark for Evaluating Large Language Models in Condensed Matter Physics](https://arxiv.org/abs/2508.08968) | CMPhysBench introduces a specialized benchmark for evaluating LLM performance in condensed matter physics, testing models' understanding of complex physical concepts, mathematical formulations, and scientific reasoning within this specific domain. The benchmark provides standardized evaluation protocols for assessing AI systems' capabilities in advanced physics applications. | Domain-Specific Evaluation | August 2025 |
| [How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation Models](https://arxiv.org/abs/2507.01622) | This paper provides systematic evaluation of GPT-4o's vision understanding capabilities across standard computer vision tasks. The evaluation covers object recognition, spatial reasoning, visual question answering, and multimodal integration, offering comprehensive insights into the current state of vision understanding in advanced multimodal foundation models. | Multimodal Evaluation | July 2025 |
| [Reasoning or Memorization? Unreliable Results of Reinforcement Learning](https://arxiv.org/abs/2507.03425) | This critical evaluation study examines potential data contamination and memorization issues in reinforcement learning evaluation, questioning the reliability of current evaluation methodologies. The research highlights fundamental challenges in distinguishing genuine reasoning from memorization in AI evaluation, providing important insights for improving evaluation reliability and validity. | Evaluation Methodologies | July 2025 |
| [Zebra-CoT: A Dataset for Interleaved Vision Language Reasoning](https://arxiv.org/abs/2507.01005) | Zebra-CoT introduces a specialized dataset and evaluation framework for assessing interleaved vision-language reasoning capabilities. The dataset tests models' ability to alternate between visual and linguistic reasoning steps, providing a more nuanced evaluation of multimodal reasoning processes and chain-of-thought capabilities in vision-language tasks. | Multimodal Evaluation | July 2025 |
| [OST-Bench: Evaluating the Capabilities of MLLMs in Online Spatio-temporal Scene Understanding](https://arxiv.org/abs/2507.03194) | OST-Bench provides a comprehensive benchmark for evaluating Multimodal Large Language Models' capabilities in understanding online spatio-temporal scenes. The benchmark tests models' ability to process dynamic visual information, understand temporal relationships, and reason about spatial-temporal changes in real-time or near-real-time scenarios. | Multimodal Evaluation | July 2025 |
| [OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models](https://arxiv.org/abs/2506.17937) | OmniSpatial creates a comprehensive spatial reasoning evaluation framework for Vision Language Models, covering various aspects of spatial understanding including 2D and 3D reasoning, spatial relationships, and geometric understanding. The benchmark provides standardized evaluation methods for measuring spatial intelligence across different VLM architectures. | Reasoning & Cognitive Evaluation | June 2025 |
| [Scientists' First Exam: Probing Cognitive Abilities of MLLM via Perception, Understanding, and Reasoning](https://arxiv.org/abs/2506.14444) | This evaluation framework probes the cognitive abilities of Multimodal Large Language Models through a comprehensive examination covering perception, understanding, and reasoning capabilities. The benchmark tests fundamental cognitive processes that mirror human scientific thinking, providing insights into how well current MLLMs replicate human-like cognitive abilities. | Reasoning & Cognitive Evaluation | June 2025 |
| [CSVQA: A Chinese Multimodal Benchmark for Evaluating STEM Reasoning Capabilities of VLMs](https://arxiv.org/abs/2506.09711) | CSVQA introduces a specialized Chinese multimodal benchmark for evaluating Vision-Language Models' STEM reasoning capabilities. The benchmark addresses the need for non-English evaluation frameworks while focusing on complex scientific, technological, engineering, and mathematical reasoning tasks that require both visual and linguistic understanding. | Domain-Specific Evaluation | June 2025 |
| [MultiFinBen: A Multilingual, Multimodal, and Difficulty-Aware Benchmark for Financial LLM Evaluation](https://arxiv.org/abs/2506.15472) | MultiFinBen provides a comprehensive evaluation framework for financial LLMs that incorporates multilingual support, multimodal capabilities, and difficulty-aware assessment. The benchmark tests financial reasoning, regulatory knowledge, and market analysis capabilities across different languages and difficulty levels, enabling more nuanced evaluation of financial AI applications. | Domain-Specific Evaluation | June 2025 |
| [KRIS-Bench: Benchmarking Next-Level Intelligent Image Editing Models](https://arxiv.org/abs/2505.20392) | KRIS-Bench introduces evaluation methodologies for next-generation intelligent image editing models, testing capabilities beyond basic editing to include contextual understanding, creative enhancement, and instruction-following in image manipulation tasks. The benchmark provides standardized evaluation for increasingly sophisticated image editing AI systems. | Multimodal Evaluation | May 2025 |
| [BizFinBench: A Business-Driven Real-World Financial Benchmark for Evaluating LLMs](https://arxiv.org/abs/2505.17972) | BizFinBench creates a business-driven evaluation framework for financial LLMs using real-world financial scenarios and business contexts. The benchmark moves beyond academic financial tasks to test models' practical applicability in actual business environments, providing more realistic assessment of financial AI capabilities. | Domain-Specific Evaluation | May 2025 |
| [MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs](https://arxiv.org/abs/2505.12437) | MME-Reasoning develops a comprehensive benchmark specifically for evaluating logical reasoning capabilities in Multimodal Large Language Models. The framework tests various forms of logical reasoning including deductive, inductive, and abductive reasoning across multimodal contexts, providing detailed assessment of reasoning capabilities beyond simple pattern recognition. | Reasoning & Cognitive Evaluation | May 2025 |
| [MMLongBench: Benchmarking Long-Context Vision-Language Models Effectively and Thoroughly](https://arxiv.org/abs/2505.08743) | MMLongBench addresses the challenge of evaluating vision-language models with long-context capabilities, providing comprehensive assessment methods for models that process extended visual and textual sequences. The benchmark tests sustained attention, coherence maintenance, and reasoning consistency across long multimodal inputs. | Multimodal Evaluation | May 2025 |
| [ScienceBoard: Evaluating Multimodal Autonomous Agents in Realistic Scientific Workflows](https://arxiv.org/abs/2505.11465) | ScienceBoard creates evaluation frameworks for multimodal autonomous agents operating in realistic scientific workflows, testing their ability to conduct research, analyze data, and generate scientific insights. The benchmark evaluates agents' performance in complex, multi-step scientific processes that require sustained reasoning and tool use. | Domain-Specific Evaluation | May 2025 |
| [PaperBench: Evaluating AI's Ability to Replicate AI Research](https://arxiv.org/abs/2504.18906) | PaperBench introduces a unique evaluation framework that tests AI systems' ability to replicate AI research processes, from literature review to experimental design and result analysis. This meta-evaluation approach provides insights into how well AI systems understand and can contribute to the research process itself. | Domain-Specific Evaluation | April 2025 |
| [PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models](https://arxiv.org/abs/2504.16915) | PHYBench provides comprehensive evaluation of Large Language Models' physical perception and reasoning capabilities, testing understanding of physical laws, cause-and-effect relationships, and real-world physics applications. The benchmark bridges the gap between abstract reasoning and practical physical understanding. | Reasoning & Cognitive Evaluation | April 2025 |
| [ColorBench: Can VLMs See and Understand the Colorful World? A Comprehensive Benchmark for Color Perception, Reasoning, and Robustness](https://arxiv.org/abs/2504.17912) | ColorBench creates a specialized evaluation framework for testing Vision-Language Models' color perception, reasoning, and robustness. The benchmark evaluates models' ability to accurately perceive colors, reason about color relationships, and maintain performance under various color-related challenges and perturbations. | Multimodal Evaluation | April 2025 |
| [xVerify: Efficient Answer Verifier for Reasoning Model Evaluations](https://arxiv.org/abs/2504.13985) | xVerify introduces an efficient verification system specifically designed for evaluating reasoning models' answer quality and correctness. The system provides automated, scalable verification methods that can assess reasoning steps, logical consistency, and final answer accuracy across various reasoning tasks and domains. | LLM Judges & Automated Evaluation | April 2025 |
| [Creation-MMBench: Assessing Context-Aware Creative Intelligence](https://arxiv.org/abs/2503.14478) | Creation-MMBench develops evaluation methodologies for assessing creative intelligence in multimodal models, focusing on context-aware creative generation and reasoning. The benchmark tests models' ability to produce novel, contextually appropriate creative outputs across various artistic and creative domains. | Multimodal Evaluation | March 2025 |
| [Challenging the Boundaries of Reasoning: An Olympiad-Level Math Benchmark](https://arxiv.org/abs/2503.21380) | This benchmark evaluates mathematical reasoning using Olympiad-level problems that require advanced mathematical insight and creative problem-solving. The evaluation framework tests mathematical reasoning capabilities in current AI systems, providing insights into current model performance on complex mathematical tasks. | Reasoning & Cognitive Evaluation | March 2025 |
| [A Comprehensive Survey on Long Context Language Modeling](https://arxiv.org/abs/2503.17407) | This comprehensive survey examines evaluation methodologies and challenges in long-context language modeling, covering assessment approaches for models that process extremely long text sequences. The survey provides systematic analysis of current evaluation practices and identifies gaps in long-context assessment methodologies. | Evaluation Surveys | March 2025 |
| [A Survey of Efficient Reasoning for Large Reasoning Models](https://arxiv.org/abs/2503.21614) | This survey explores evaluation approaches for efficient reasoning in large-scale reasoning models, examining trade-offs between reasoning quality and computational efficiency. The work provides comprehensive analysis of current evaluation methodologies for measuring reasoning efficiency and effectiveness in resource-constrained environments. | Evaluation Surveys | March 2025 |
| [MMTEB: Massive Multilingual Text Embedding Benchmark](https://arxiv.org/abs/2502.05823) | MMTEB introduces a massive multilingual benchmark for evaluating text embedding models across diverse languages and tasks. The benchmark provides standardized evaluation protocols for measuring embedding quality, cross-lingual transfer capabilities, and performance consistency across different linguistic contexts and cultural domains. | Benchmarks & Datasets | February 2025 |
| [BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models](https://arxiv.org/abs/2502.09485) | BenchMAX creates a multilingual evaluation suite that assesses LLM performance across diverse languages, cultures, and linguistic phenomena. The benchmark addresses inclusive AI evaluation that extends beyond English-centric assessment to evaluate AI system performance across different languages and populations. | Benchmarks & Datasets | February 2025 |
| [On the Trustworthiness of Generative Foundation Models: Guideline, Assessment, and Perspective](https://arxiv.org/abs/2502.04821) | This paper provides comprehensive guidelines and assessment frameworks for evaluating the trustworthiness of generative foundation models, covering reliability, fairness, transparency, and safety aspects. The work establishes evaluation standards for ensuring responsible deployment of generative AI systems across various applications. | Robustness & Safety Evaluation | February 2025 |
| [URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics](https://arxiv.org/abs/2501.12973) | URSA develops evaluation methodologies for understanding and verifying chain-of-thought reasoning in multimodal mathematical contexts. The framework provides tools for assessing the quality, correctness, and logical coherence of reasoning chains that integrate visual and textual mathematical information. | Reasoning & Cognitive Evaluation | January 2025 |
| [Multiple Choice Questions: Reasoning Makes Large Language Models More Self-Confident Even When They Are Wrong](https://arxiv.org/abs/2501.08734) | This study evaluates the relationship between reasoning processes and model confidence in multiple choice scenarios, showing how reasoning can increase model self-confidence even for incorrect answers. The research provides insights into evaluation methodologies that account for confidence calibration and reasoning-confidence interactions. | Evaluation Methodologies | January 2025 |
| [Redundancy Principles for MLLMs Benchmarks](https://arxiv.org/abs/2501.15642) | This paper establishes principles for creating effective MLLM benchmarks by addressing redundancy issues in evaluation datasets. The work provides guidelines for designing more efficient and comprehensive evaluation frameworks that avoid redundant testing while maintaining thorough assessment coverage. | Evaluation Methodologies | January 2025 |
| [Reasoning Language Models: A Blueprint](https://arxiv.org/abs/2501.13502) | This blueprint paper provides comprehensive framework for understanding and evaluating reasoning capabilities in language models, offering systematic approaches for designing reasoning-focused evaluation methodologies. The work serves as a foundational guide for developing robust reasoning assessment frameworks. | Evaluation Methodologies | January 2025 |
| [InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model](https://arxiv.org/abs/2501.07945) | This paper introduces a multimodal reward model designed for automated evaluation of AI system outputs, providing efficient assessment methods for multimodal generation tasks. The reward model offers scalable evaluation solutions for measuring quality and alignment in multimodal AI applications. | LLM Judges & Automated Evaluation | January 2025 |
| [LLM4SR: A Survey on Large Language Models for Scientific Research](https://arxiv.org/abs/2501.11946) | LLM4SR provides a comprehensive survey of evaluation methodologies for Large Language Models in scientific research applications, examining assessment approaches across various scientific domains and research workflows. The survey serves as a guide for evaluating AI systems designed to support scientific discovery and research processes. | Evaluation Surveys | January 2025 |
| [RealCritic: Towards Effectiveness-Driven Evaluation of Language Model Critiques](https://arxiv.org/abs/2501.09136) | RealCritic addresses the challenge of evaluating language models' critique capabilities by introducing an effectiveness-driven evaluation framework. The approach moves beyond traditional evaluation methods to assess how well models can provide useful, actionable feedback and critiques. By focusing on the practical effectiveness of model-generated critiques, RealCritic provides insights into models' ability to identify issues, suggest improvements, and provide meaningful feedback in various contexts. | LLM Judges & Automated Evaluation | January 2025 |